{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a973ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models.video as video_models\n",
    "import openl3\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torchvision.io import read_video\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798016dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, proj_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(proj_dim, proj_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a35455",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoEncoder(nn.Module):\n",
    "    def __init__(self, proj_dim=128, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        base_model = video_models.r3d_18(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])  # remove final FC\n",
    "        self.proj_head = ProjectionHead(512, proj_dim)\n",
    "\n",
    "        if freeze_backbone:\n",
    "            for param in self.feature_extractor.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):  # (B, C, T, H, W)\n",
    "        x = self.feature_extractor(x)  # (B, 512, 1, 1, 1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.proj_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d1b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenL3Encoder(nn.Module):\n",
    "    def __init__(self, proj_dim=128, input_repr=\"mel256\", content_type=\"music\", embedding_size=512):\n",
    "        super().__init__()\n",
    "        self.sr = 48000\n",
    "        self.model = openl3.models.load_audio_embedding_model(\n",
    "            input_repr=input_repr,\n",
    "            content_type=content_type,\n",
    "            embedding_size=embedding_size\n",
    "        )\n",
    "\n",
    "        self.proj_head = ProjectionHead(embedding_size, proj_dim)\n",
    "\n",
    "    def forward(self, x):  # x: (B, L)\n",
    "        embeddings = []\n",
    "        batch_size = len(x) # Get batch size from list length\n",
    "\n",
    "        # Check if the batch is empty\n",
    "        if batch_size == 0:\n",
    "            # Handle empty batch: maybe return an empty tensor of the expected shape\n",
    "            # The projection head expects input dimension embedding_size\n",
    "            # Example: return an empty tensor for the projection head input\n",
    "             empty_proj_input = torch.empty(0, self.proj_head.net[0].in_features)\n",
    "             # If x was expected to be on a specific device, match it if possible, else use default\n",
    "             device = self.proj_head.net[0].weight.device # Get device from proj_head params\n",
    "             empty_proj_input = empty_proj_input.to(device)\n",
    "             return self.proj_head(empty_proj_input)\n",
    "\n",
    "\n",
    "        # Iterate through the list of tensors\n",
    "        for i in range(batch_size):\n",
    "            audio_tensor = x[i] # Get the i-th tensor\n",
    "            # Ensure tensor is 1D numpy array for openl3\n",
    "            # Squeeze potentially removes channel dim if present (e.g., [1, L] -> [L])\n",
    "            audio_np = audio_tensor.squeeze().detach().cpu().numpy()\n",
    "\n",
    "            # Check if audio_np is actually 1D after squeeze\n",
    "            if audio_np.ndim != 1:\n",
    "                # Handle unexpected dimensions, e.g. could be empty after processing\n",
    "                print(f\"Warning: Audio sample {i} has unexpected shape {audio_np.shape} after processing. Skipping.\")\n",
    "                # Option: append a zero tensor or skip. Skipping requires careful handling later.\n",
    "                # For simplicity, let's try to add zeros if shape is bad, but this might hide issues.\n",
    "                # A better approach might be to ensure the dataset always returns valid 1D audio.\n",
    "                # Assuming embedding_size is the dimension needed:\n",
    "                emb_mean = torch.zeros(self.proj_head.net[0].in_features, device=audio_tensor.device).float()\n",
    "\n",
    "            elif audio_np.size == 0:\n",
    "                 print(f\"Warning: Audio sample {i} is empty after processing. Skipping.\")\n",
    "                 emb_mean = torch.zeros(self.proj_head.net[0].in_features, device=audio_tensor.device).float()\n",
    "            else:\n",
    "                # Process with OpenL3 using the correct model attribute name\n",
    "                emb, _ = openl3.get_audio_embedding(audio_np, self.sr, model=self.model, center=True)\n",
    "                # Aggregate embeddings and move to the correct device\n",
    "                emb_mean = torch.tensor(emb.mean(axis=0), device=audio_tensor.device).float()\n",
    "\n",
    "            embeddings.append(emb_mean)\n",
    "\n",
    "        # Stack embeddings for the batch\n",
    "        embeddings_batch = torch.stack(embeddings)  # (B, embedding_size)\n",
    "\n",
    "        # Apply the PyTorch projection head\n",
    "        return self.proj_head(embeddings_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44854e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AVContrastiveModel(nn.Module):\n",
    "    def __init__(self, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.video_encoder = VideoEncoder(proj_dim, freeze_backbone=True)\n",
    "        self.audio_encoder = OpenL3Encoder(proj_dim)\n",
    "\n",
    "    def forward(self, video, audio):\n",
    "        z_video = self.video_encoder(video)  # (B, proj_dim)\n",
    "        z_audio = self.audio_encoder(audio)  # (B, proj_dim)\n",
    "        return z_video, z_audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8f3519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(z1, z2, temperature=0.07):\n",
    "    # z1, z2 shapes: (B, D)\n",
    "    B = z1.size(0)\n",
    "\n",
    "    # Normalize features\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "\n",
    "    # Concatenate features: Video embeddings first, then Audio embeddings\n",
    "    z = torch.cat([z1, z2], dim=0)  # Shape: (2B, D)\n",
    "\n",
    "    # Calculate pairwise cosine similarity\n",
    "    # sim[i, j] = similarity between z[i] and z[j]\n",
    "    sim = torch.matmul(z, z.T)  # Shape: (2B, 2B)\n",
    "\n",
    "    # --- REMOVE MASKING AND RESHAPING ---\n",
    "    # mask = ~torch.eye(2 * B, dtype=bool, device=z.device)\n",
    "    # logits = sim[mask].view(2 * B, -1) / temperature # Incorrect shape (2B, 2B-1)\n",
    "    # --- FIX: Use the full similarity matrix ---\n",
    "    logits = sim / temperature # Shape: (2B, 2B)\n",
    "\n",
    "    # Create targets:\n",
    "    # For the first B rows (videos z1), the positive match is the corresponding audio (z2) at index i+B\n",
    "    # For the second B rows (audios z2), the positive match is the corresponding video (z1) at index i\n",
    "    targets_arange = torch.arange(B, device=z.device)\n",
    "    # Targets for rows 0 to B-1 should be B to 2B-1\n",
    "    # Targets for rows B to 2B-1 should be 0 to B-1\n",
    "    targets = torch.cat([targets_arange + B, targets_arange]) # Shape: (2B,) Correct indices for (2B, 2B) logits\n",
    "\n",
    "    # Calculate cross-entropy loss\n",
    "    # logits shape (2B, 2B), targets shape (2B,) with values in [0, 2B-1]\n",
    "    return F.cross_entropy(logits, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2777d893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss_with_masking(z1, z2, temperature=0.07):\n",
    "    # z1, z2 shapes: (B, D)\n",
    "    B = z1.size(0)\n",
    "\n",
    "    # Normalize features to unit length (cosine similarity)\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "\n",
    "    # Concatenate video and audio features\n",
    "    z = torch.cat([z1, z2], dim=0)  # Shape: (2B, D)\n",
    "\n",
    "    # Compute pairwise similarity matrix (cosine sim / temperature)\n",
    "    sim = torch.matmul(z, z.T) / temperature  # Shape: (2B, 2B)\n",
    "\n",
    "    # Mask self-similarity (prevent matching an embedding with itself)\n",
    "    mask = torch.eye(2 * B, dtype=torch.bool, device=z.device)\n",
    "    sim = sim.masked_fill(mask, -1e9)  # Assign large negative value so softmax ~ 0\n",
    "\n",
    "    # Construct targets:\n",
    "    # Positives are at positions: [i → i+B] and [i+B → i]\n",
    "    targets = torch.arange(B, device=z.device)\n",
    "    targets = torch.cat([targets + B, targets])  # Shape: (2B,)\n",
    "\n",
    "    # Cross-entropy loss over rows\n",
    "    return F.cross_entropy(sim, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AVContrastiveDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir=\"clipped_data\",      # Original data structure (for finding samples)\n",
    "        aug_root=\"augmented_data\",    # Root for augmented files\n",
    "        video_aug_dirs=[\"crop_color\", \"crop_sobel\"], # Subdirs under aug_root\n",
    "        audio_aug_dirs=[\"bg_noise\", \"drc\"],          # Subdirs under aug_root\n",
    "        num_frames=16,\n",
    "        video_size=(112, 112),\n",
    "        audio_sr=48000\n",
    "    ):\n",
    "        self.root = Path(root_dir)\n",
    "        self.aug_root = Path(aug_root)\n",
    "        self.video_aug_dirs = video_aug_dirs\n",
    "        self.audio_aug_dirs = audio_aug_dirs\n",
    "        self.num_frames = num_frames\n",
    "        self.video_size = video_size\n",
    "        self.audio_sr = audio_sr\n",
    "\n",
    "        # Collect all .mp4 files under class folders in the ORIGINAL directory\n",
    "        self.samples = []\n",
    "        # Ensure root directory exists\n",
    "        if not self.root.is_dir():\n",
    "             raise FileNotFoundError(f\"Root directory '{self.root}' not found.\")\n",
    "\n",
    "        self.classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])\n",
    "        if not self.classes:\n",
    "             print(f\"Warning: No class subdirectories found in '{self.root}'. Dataset will be empty.\")\n",
    "\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "\n",
    "        for cls in self.classes:\n",
    "            class_path = self.root / cls\n",
    "            if not class_path.is_dir():\n",
    "                print(f\"Warning: Expected directory, but found file: {class_path}\")\n",
    "                continue\n",
    "            found_files = False\n",
    "            for video_path in class_path.glob(\"*.mp4\"):\n",
    "                base_name = video_path.stem\n",
    "                self.samples.append((cls, base_name)) # Store class and base filename\n",
    "                found_files = True\n",
    "            if not found_files:\n",
    "                 print(f\"Warning: No .mp4 files found in class directory: {class_path}\")\n",
    "\n",
    "\n",
    "        if not self.samples:\n",
    "             print(f\"Warning: No samples collected. Check '{self.root}' structure and content.\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the class name (e.g., 'violin') and base filename (e.g., 'video_001')\n",
    "        cls, base = self.samples[idx]\n",
    "        label = self.class_to_idx[cls]\n",
    "\n",
    "        # Choose random augmentations\n",
    "        video_aug = random.choice(self.video_aug_dirs)\n",
    "        audio_aug = random.choice(self.audio_aug_dirs)\n",
    "\n",
    "        # --- CORRECTED PATH CONSTRUCTION ---\n",
    "        # Assumes structure: augmented_data/<aug_type>/<class>/<base_name>.mp4\n",
    "        video_path = self.aug_root / video_aug / cls / f\"{base}.mp4\"\n",
    "        audio_path = self.aug_root / audio_aug / cls / f\"{base}.wav\"\n",
    "        # --- END CORRECTION ---\n",
    "\n",
    "        try:\n",
    "            # Load video\n",
    "            video, _, _ = read_video(str(video_path), pts_unit=\"sec\")\n",
    "\n",
    "            # Check for empty video immediately after loading\n",
    "            if video.nelement() == 0:\n",
    "                print(f\"WARNING: Loaded video tensor is empty for {video_path}!\")\n",
    "                # Decide how to handle: skip, raise error, return dummy?\n",
    "                # For now, raising an error is safest to alert you.\n",
    "                raise RuntimeError(f\"Video file {video_path} loaded with 0 frames or elements.\")\n",
    "\n",
    "            video = video.permute(0, 3, 1, 2).float() / 255.0  # T x C x H x W\n",
    "\n",
    "            T_total = video.size(0)\n",
    "\n",
    "            # Explicitly check for T_total <= 0 before division/modulo\n",
    "            if T_total <= 0:\n",
    "                 raise RuntimeError(f\"Video file {video_path} resulted in T_total={T_total} after permute.\")\n",
    "\n",
    "            # Resize and crop video frames\n",
    "            if T_total > self.num_frames:\n",
    "                start = random.randint(0, T_total - self.num_frames)\n",
    "                video = video[start : start + self.num_frames]\n",
    "            else:\n",
    "                # Now T_total > 0 is guaranteed here\n",
    "                repeat = (self.num_frames + T_total - 1) // T_total\n",
    "                video = video.repeat((repeat, 1, 1, 1))[:self.num_frames]\n",
    "\n",
    "            video = torch.nn.functional.interpolate(video, size=self.video_size, mode='bilinear', align_corners=False) # Added align_corners=False often recommended\n",
    "            video = video.permute(1, 0, 2, 3)  # → C x T x H x W\n",
    "\n",
    "            # Load and resample audio\n",
    "            waveform, sr = torchaudio.load(str(audio_path))\n",
    "            if waveform.shape[0] > 1:\n",
    "                waveform = waveform.mean(dim=0, keepdim=True)  # Mono\n",
    "            if sr != self.audio_sr:\n",
    "                resampler = torchaudio.transforms.Resample(sr, self.audio_sr)\n",
    "                waveform = resampler(waveform)\n",
    "            # Ensure waveform is 1D (B, L) -> (L) for OpenL3 later? Check model input.\n",
    "            # The OpenL3Encoder seems to handle batching internally by iterating,\n",
    "            # so returning individual waveforms might be correct.\n",
    "            # waveform = waveform.squeeze(0) # If needed\n",
    "\n",
    "        except FileNotFoundError:\n",
    "             print(f\"ERROR: File not found at path: {video_path} or {audio_path}\")\n",
    "             # Depending on desired behavior, you might want to return None\n",
    "             # and handle it in the DataLoader's collate_fn, or raise the error.\n",
    "             raise # Re-raise the error for now\n",
    "        except Exception as e:\n",
    "            print(f\"ERROR processing sample idx {idx} (cls='{cls}', base='{base}') at path {video_path} / {audio_path}: {e}\")\n",
    "            raise # Re-raise the error\n",
    "\n",
    "        return video, waveform.squeeze(0), label # Return waveform likely needs to be (L,) not (1, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b4c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume all model components are defined:\n",
    "# - AVContrastiveModel\n",
    "# - contrastive_loss\n",
    "# - AVContrastiveDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = AVContrastiveModel(proj_dim=128).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = AVContrastiveDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=0)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (video, audio, _) in enumerate(dataloader):\n",
    "        # video: (B, C, T, H, W), audio: (B, L)\n",
    "        video = video.to(device)\n",
    "        audio = [a.to(device) for a in audio]  # individual waveforms (already variable length)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        z_video, z_audio = model(video, audio)  # OpenL3 handles audio per-sample\n",
    "        loss = contrastive_loss(z_video, z_audio)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print(f\"[Epoch {epoch+1}] Step {i}/{len(dataloader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"[Epoch {epoch+1}] Avg Loss: {avg_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
