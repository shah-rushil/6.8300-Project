{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5a973ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models.video as video_models\n",
    "import openl3\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "from torchvision.io import read_video\n",
    "import torchaudio\n",
    "from pathlib import Path\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798016dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, in_dim, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, proj_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(proj_dim, proj_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a35455",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoEncoder(nn.Module):\n",
    "    def __init__(self, proj_dim=128):\n",
    "        super().__init__()\n",
    "        base_model = video_models.r3d_18(pretrained=True)\n",
    "        self.feature_extractor = nn.Sequential(*list(base_model.children())[:-1])  # remove final FC\n",
    "        self.proj_head = ProjectionHead(512, proj_dim)\n",
    "\n",
    "    def forward(self, x):  # (B, C, T, H, W)\n",
    "        x = self.feature_extractor(x)  # (B, 512, 1, 1, 1)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.proj_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43d1b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenL3Encoder(nn.Module):\n",
    "    def __init__(self, proj_dim=128, input_repr=\"mel256\", content_type=\"music\", embedding_size=512):\n",
    "        super().__init__()\n",
    "        self.sr = 48000\n",
    "        self.model = openl3.models.load_audio_embedding_model(\n",
    "            input_repr=input_repr,\n",
    "            content_type=content_type,\n",
    "            embedding_size=embedding_size\n",
    "        )\n",
    "        self.model.eval()\n",
    "        for p in self.model.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.proj_head = ProjectionHead(embedding_size, proj_dim)\n",
    "\n",
    "    def forward(self, x):  # x: (B, L)\n",
    "        embeddings = []\n",
    "        for i in range(x.size(0)):\n",
    "            audio_np = x[i].detach().cpu().numpy()\n",
    "            emb, _ = openl3.get_audio_embedding(audio_np, self.sr, model=self.model, center=True)\n",
    "            emb_mean = torch.tensor(emb.mean(axis=0), device=x.device).float()\n",
    "            embeddings.append(emb_mean)\n",
    "        embeddings = torch.stack(embeddings)  # (B, 512)\n",
    "        return self.proj_head(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44854e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AVContrastiveModel(nn.Module):\n",
    "    def __init__(self, proj_dim=128):\n",
    "        super().__init__()\n",
    "        self.video_encoder = VideoEncoder(proj_dim)\n",
    "        self.audio_encoder = OpenL3Encoder(proj_dim)\n",
    "\n",
    "    def forward(self, video, audio):\n",
    "        z_video = self.video_encoder(video)  # (B, proj_dim)\n",
    "        z_audio = self.audio_encoder(audio)  # (B, proj_dim)\n",
    "        return z_video, z_audio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8f3519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contrastive_loss(z1, z2, temperature=0.07):\n",
    "    z1 = F.normalize(z1, dim=1)\n",
    "    z2 = F.normalize(z2, dim=1)\n",
    "    z = torch.cat([z1, z2], dim=0)  # (2B, D)\n",
    "\n",
    "    sim = torch.matmul(z, z.T)  # cosine similarity\n",
    "    B = z1.size(0)\n",
    "    mask = ~torch.eye(2 * B, dtype=bool, device=z.device)\n",
    "\n",
    "    # Similarity with negatives masked out\n",
    "    logits = sim[mask].view(2 * B, -1) / temperature\n",
    "    targets = torch.arange(B, device=z.device)\n",
    "    targets = torch.cat([targets + B, targets])\n",
    "\n",
    "    return F.cross_entropy(logits, targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227d6e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AVContrastiveDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root_dir=\"clipped_data\",\n",
    "        aug_root=\"augmented_data\",\n",
    "        video_aug_dirs=[\"crop_color\", \"crop_sobel\"],\n",
    "        audio_aug_dirs=[\"bg_noise\", \"drc\"],\n",
    "        num_frames=16,\n",
    "        video_size=(112, 112),\n",
    "        audio_sr=48000\n",
    "    ):\n",
    "        self.root = Path(root_dir)\n",
    "        self.aug_root = Path(aug_root)\n",
    "        self.video_aug_dirs = video_aug_dirs\n",
    "        self.audio_aug_dirs = audio_aug_dirs\n",
    "        self.num_frames = num_frames\n",
    "        self.video_size = video_size\n",
    "        self.audio_sr = audio_sr\n",
    "\n",
    "        # Collect all .mp4 files under class folders\n",
    "        self.samples = []\n",
    "        self.classes = sorted([d.name for d in self.root.iterdir() if d.is_dir()])\n",
    "        self.class_to_idx = {cls: i for i, cls in enumerate(self.classes)}\n",
    "\n",
    "        for cls in self.classes:\n",
    "            for video_path in (self.root / cls).glob(\"*.mp4\"):\n",
    "                base_name = video_path.stem\n",
    "                self.samples.append((cls, base_name))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cls, base = self.samples[idx]\n",
    "        label = self.class_to_idx[cls]\n",
    "\n",
    "        # Choose random augmentations\n",
    "        video_aug = random.choice(self.video_aug_dirs)\n",
    "        audio_aug = random.choice(self.audio_aug_dirs)\n",
    "\n",
    "        # Construct paths\n",
    "        video_path = self.aug_root / video_aug / f\"{base}.mp4\"\n",
    "        audio_path = self.aug_root / audio_aug / f\"{base}.wav\"\n",
    "\n",
    "        # Load video\n",
    "        video, _, _ = read_video(str(video_path), pts_unit=\"sec\")\n",
    "        video = video.permute(0, 3, 1, 2).float() / 255.0  # T x C x H x W\n",
    "\n",
    "        # Resize and crop (manual resize to match ResNet input)\n",
    "        T_total = video.size(0)\n",
    "        if T_total > self.num_frames:\n",
    "            start = random.randint(0, T_total - self.num_frames)\n",
    "            video = video[start : start + self.num_frames]\n",
    "        else:\n",
    "            repeat = (self.num_frames + T_total - 1) // T_total\n",
    "            video = video.repeat((repeat, 1, 1, 1))[:self.num_frames]\n",
    "\n",
    "        video = torch.nn.functional.interpolate(video, size=self.video_size, mode='bilinear')\n",
    "        video = video.permute(1, 0, 2, 3)  # â†’ C x T x H x W\n",
    "\n",
    "        # Load and resample audio\n",
    "        waveform, sr = torchaudio.load(audio_path)\n",
    "        if waveform.shape[0] > 1:\n",
    "            waveform = waveform.mean(dim=0, keepdim=True)  # Mono\n",
    "        if sr != self.audio_sr:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.audio_sr)\n",
    "            waveform = resampler(waveform)\n",
    "\n",
    "        return video, waveform, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6b4c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume all model components are defined:\n",
    "# - AVContrastiveModel\n",
    "# - contrastive_loss\n",
    "# - AVContrastiveDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model and optimizer\n",
    "model = AVContrastiveModel(proj_dim=128).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = AVContrastiveDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (video, audio, _) in enumerate(dataloader):\n",
    "        # video: (B, C, T, H, W), audio: (B, L)\n",
    "        video = video.to(device)\n",
    "        audio = [a.to(device) for a in audio]  # individual waveforms (already variable length)\n",
    "\n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        z_video, z_audio = model(video, audio)  # OpenL3 handles audio per-sample\n",
    "        loss = contrastive_loss(z_video, z_audio)\n",
    "\n",
    "        # Backward and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        if i % 10 == 0:\n",
    "            print(f\"[Epoch {epoch+1}] Step {i}/{len(dataloader)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"[Epoch {epoch+1}] Avg Loss: {avg_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
