<html>

<head>

	<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>

	<link rel="shortcut icon" href="images/icon.ico">
	<style type="text/css">
		body {
			background-color: #f5f9ff;
		}

		/* Hide both math displays initially, will display based on JS detection */
		.mathjax-mobile,
		.mathml-non-mobile {
			display: none;
		}

		/* Show the MathML content by default on non-mobile devices */
		.show-mathml .mathml-non-mobile {
			display: block;
		}

		.show-mathjax .mathjax-mobile {
			display: block;
		}

		.content-margin-container {
			display: flex;
			width: 100%;
			/* Ensure the container is full width */
			justify-content: left;
			/* Horizontally centers the children in the container */
			align-items: center;
			/* Vertically centers the children in the container */
		}

		.main-content-block {
			width: 70%;
			/* Change this percentage as needed */
			max-width: 1100px;
			/* Optional: Maximum width */
			background-color: #fff;
			border-left: 1px solid #DDD;
			border-right: 1px solid #DDD;
			padding: 8px 8px 8px 8px;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		}

		.margin-left-block {
			font-size: 14px;
			width: 15%;
			/* Change this percentage as needed */
			max-width: 130px;
			/* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			padding: 5px;
		}

		.margin-right-block {
			font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
			font-size: 14px;
			width: 25%;
			/* Change this percentage as needed */
			max-width: 256px;
			/* Optional: Maximum width */
			position: relative;
			text-align: left;
			padding: 10px;
			/* Optional: Adds padding inside the caption */
		}

		img {
			max-width: 100%;
			/* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
		}

		.my-video {
			max-width: 100%;
			/* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
		}

		/* Hide both video displays initially, will display based on JS detection */
		.vid-mobile,
		.vid-non-mobile {
			display: none;
		}

		/* Show the video content by default on non-mobile devices */
		.show-vid-mobile .vid-mobile {
			display: block;
		}

		.show-vid-non-mobile .vid-non-mobile {
			display: block;
		}

		a:link,
		a:visited {
			color: #0e7862;
			/*#1367a7;*/
			text-decoration: none;
		}

		a:hover {
			color: #24b597;
			/*#208799;*/
		}

		h1 {
			font-size: 24px;
			margin-top: 4px;
			margin-bottom: 10px;
		}

		h2 {
			font-size: 18px;
			margin-top: 4px;
			margin-bottom: 10px;
		}

		table.header {
			font-weight: 300;
			font-size: 17px;
			flex-grow: 1;
			width: 70%;
			max-width: calc(100% - 290px);
			/* Adjust according to the width of .paper-code-tab */
		}

		table td,
		table td * {
			vertical-align: middle;
			position: relative;
		}

		table.paper-code-tab {
			flex-shrink: 0;
			margin-left: 8px;
			margin-top: 8px;
			padding: 0px 0px 0px 8px;
			width: 290px;
			height: 150px;
		}

		.layered-paper {
			/* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
			box-shadow:
				0px 0px 1px 1px rgba(0, 0, 0, 0.35),
				/* The top layer shadow */
				5px 5px 0 0px #fff,
				/* The second layer */
				5px 5px 1px 1px rgba(0, 0, 0, 0.35),
				/* The second layer shadow */
				10px 10px 0 0px #fff,
				/* The third layer */
				10px 10px 1px 1px rgba(0, 0, 0, 0.35);
			/* The third layer shadow */
			margin-top: 5px;
			margin-left: 10px;
			margin-right: 30px;
			margin-bottom: 5px;
		}

		hr {
			height: 1px;
			/* Sets the height of the line to 1 pixel */
			border: none;
			/* Removes the default border */
			background-color: #DDD;
			/* Sets the line color to black */
		}

		div.hypothesis {
			width: 80%;
			background-color: #EEE;
			border: 1px solid black;
			border-radius: 10px;
			-moz-border-radius: 10px;
			-webkit-border-radius: 10px;
			font-family: Courier;
			font-size: 18px;
			text-align: center;
			margin: auto;
			padding: 16px 16px 16px 16px;
		}

		div.citation {
			font-size: 0.8em;
			background-color: #fff;
			padding: 10px;
			height: 200px;
		}

		.fade-in-inline {
			position: absolute;
			text-align: center;
			margin: auto;
			-webkit-mask-image: linear-gradient(to right,
					transparent 0%,
					transparent 40%,
					black 50%,
					black 90%,
					transparent 100%);
			mask-image: linear-gradient(to right,
					transparent 0%,
					transparent 40%,
					black 50%,
					black 90%,
					transparent 100%);
			-webkit-mask-size: 8000% 100%;
			mask-size: 8000% 100%;
			animation-name: sweepMask;
			animation-duration: 4s;
			animation-iteration-count: infinite;
			animation-timing-function: linear;
			animation-delay: -1s;
		}

		.fade-in2-inline {
			animation-delay: 1s;
		}

		.inline-div {
			position: relative;
			display: inline-block;
			/* Makes both the div and paragraph inline-block elements */
			vertical-align: top;
			/* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
			width: 50px;
			/* Optional: Adds space between the div and the paragraph */
		}

		.column {
			float: left;
			width: 45%;
			padding-left: 30px;
		}

		/* Clear floats after image containers */
		.row::after {
			content: "";
			clear: both;
			display: table;
		}

		.table {
			margin: 0 auto;
			/* top/bottom = 0, left/right = auto */
			border-collapse: collapse;
		}

		.th,
		.td {
			padding: 8px;
			border: 1px solid #ccc;
		}

		.container {
			display: flex;
			/* Enables Flexbox */
			align-items: flex-start;
			/* Aligns items vertically to the top (can use center, flex-end, etc.) */
			gap: 1em;
			/* Adds space between the image and text (adjust as needed) */
		}

		.container img {
			/* Optional: Prevent image from shrinking if container is too narrow */
			flex-shrink: 0;
		}

		.text-content {
			margin: 0;
			/* Removes default paragraph margin if needed */
		}

		/* Optional: Make it stack vertically on smaller screens */
		@media (max-width: 600px) {
			.container {
				flex-direction: column;
				/* Stack items vertically */
				align-items: center;
				/* Center items when stacked */
			}
		}
	</style>

	<title>Contrastive Learning of Augmented Audio-Visual Data</title>
	<meta property="og:title" content="Contrastive Learning of Augmented Audio-Visual Data" />
	<meta charset="UTF-8">
</head>

<body>

	<div class="content-margin-container">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<table class="header" align=left>
				<tr>
					<td colspan=4>
						<span
							style="font-size: 32px; font-family: 'Courier New', Courier, monospace; /* Adds fallbacks */">
							Contrastive Learning of Augmented Audio-Visual Data</span>
					</td>
				</tr>
				<tr>
					<td align=left>
						<span style="font-size:17px"><a href="https://shah-rushil.github.io">Rushil Shah</a></span>
					</td>
					<td align=left>
						<span style="font-size:17px"><a href="your_partner's_website">Alex He</a></span>
					</td>
				<tr>
					<td colspan=4 align=left><span style="font-size:18px">Final project for 6.8300, MIT</span></td>
				</tr>
			</table>
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="intro">
		<div class="margin-left-block">
			<!-- table of contents here -->
			<div style="position:fixed; max-width:inherit; top:max(20%,120px)">
				<b style="font-size:16px">Outline</b><br><br>
				<a href="#intro">Introduction</a><br><br>
				<a href="#background">Background</a><br><br>
				<a href="#methods">Methods</a><br><br>
				<a href="#analysis">Analysis</a><br><br>
				<a href="#conclusion">Conclusion</a><br><br>
				<a href="#citations">References</a><br><br>
			</div>
		</div>
		<div class="main-content-block">
			<!--You can embed an image like this:-->
			<img src="./images/your_image_here.png" width=512px />
		</div>
		<div class="margin-right-block">
			Caption for the image.
		</div>
	</div>

	<div class="content-margin-container" id="intro">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h1>Introduction</h1>
			In recent years, contrastive learning has emerged as a powerful, self-supervised technique to learn
			meaningful representations, usually by encouraging invariance to some sort of data augmentation.
			At the same time, multimodal learning has shown promise for getting a more robust understanding of complex
			data.
			By integrating information from varying modalities, models are able to combine different information to
			achieve better performance on downstream tasks.
			We propose leveraging a combination of these two in order to learn more robust representations to improve
			video and sound classification.
		</div>
		<div class="margin-right-block">
			Margin note that clarifies some detail #main-content-block for intro section.
		</div>
	</div>

	<div class="content-margin-container" id="background">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h1>Background</h1>
			Contrastive learning is a representation learning technique focused on generating similar encodings for data
			that represent similar concepts.
			To accomplish this, it utilizes the notion of positive and negative samples to optimize a "constrastive loss
			function" that rewards similar representations for similar data and penalizes similar representations of
			different data.
			For example, in their paper focusing on contrastive loss between augmented images, Chen et al. <a
				href="#ChenContrastiveLearning">[3]</a> uses the following contrastive loss function,

			<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
				<mrow>
					<msub>
						<mi>l</mi>
						<mrow>
							<mi>i</mi>
							<mo>,</mo>
							<mi>j</mi>
						</mrow>
					</msub>
					<mo>=</mo>
					<mo>&#x2212;</mo>
					<mfrac>
						<mrow>
							<mi>exp</mi>
							<mo>(</mo>
							<mi>sim</mi>
							<mo>(</mo>
							<msub>
								<mi mathvariant="bold">z</mi>
								<mi>i</mi>
							</msub>
							<mo>,</mo>
							<msub>
								<mi mathvariant="bold">z</mi>
								<mi>j</mi>
							</msub>
							<mo>)</mo>
							<mo>/</mo>
							<mi>&tau;</mi>
							<mo>)</mo>
						</mrow>
						<mrow>
							<munderover>
								<mo>∑</mo>
								<mrow>
									<mi>k</mi>
									<mo>=</mo>
									<mn>1</mn>
								</mrow>
								<mrow>
									<mn>2</mn>
									<mi>N</mi>
								</mrow>
							</munderover>
							<msub>
								<mi mathvariant="double-struck">1</mi>
								<mrow>
									<mo>[</mo>
									<mi>k</mi>
									<mo>&ne;</mo>
									<mi>i</mi>
									<mo>]</mo>
								</mrow>
							</msub>
							<mi>exp</mi>
							<mo>(</mo>
							<mi>sim</mi>
							<mo>(</mo>
							<msub>
								<mi mathvariant="bold">z</mi>
								<mi>i</mi>
							</msub>
							<mo>,</mo>
							<msub>
								<mi mathvariant="bold">z</mi>
								<mi>j</mi>
							</msub>
							<mo>)</mo>
							<mo>/</mo>
							<mi>&tau;</mi>
							<mo>),</mo>
						</mrow>
					</mfrac>
				</mrow>
			</math>
			<br>
			Where we have <i>z<sub>i</sub></i> and <i>z<sub>j</sub></i> as embeddings of samples <i>i</i> and <i>j</i>, 
			<i>sim(z<sub>i</sub>, z<sub>j</sub>)</i> calculates the cosine similarity of the embeddings, 
			<i>&tau;</i> is a temperature hyperparameter, and 
			<i>l<sub>(i, j)</sub></i> is the loss between samples <i>i</i> and <i>j</i>.
			<br>
			<br>
			Contrastive learning itself has seen usage with a variety of modalities.
			Frameworks like SimCLR augment images, extract representations from the augmented data, and apply a
			contrastive loss function to create image representations that are a significant improvement over previous
			self-supervised, semi-supervised, and transfer learning methods <a href="#ChenContrastiveLearning">[3]</a>.
			<br>
			<br>
			Though not directly contrastive learning, the audio modality has also seen augmentations such as shifting
			pitch, time stretching, noise, and volume adjustment <a href="#AudioAugmentation">[4]</a>.
			An example of the pitch shifting augmentation is demonstrated below.
			<img src="./images/AudioAugmentationExample.png" width=512px />
			Cross-modality contrastive learning has also been done, where Ma et al. propose an actively sampled
			dictionary approach to a cross-modal, audio/visual extension of the traditional contrastive learning method
			<a href="#AudioVisualConstrastiveLearning">[2]</a>.
			However, because Ma et al. focus on an actively sampled dictionary, they don't deal with data augmentation
			in their paper.
			This is what we plan to explore in ours - augmentations of audio and visual data and the effect they can
			have on learning effective representations.
		</div>
		<div class="margin-right-block" style="transform: translate(0%,30%);">
			The decrease (-) and increase (+) in pitch for a rooster crowing and its effects on the log-mel spectrogram.
		</div>
	</div>
	</div>

	<div class="content-margin-container" id="methods">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h1>Methods</h1>
			Formally, we want to understand if augmenting audio and visual data help models learn more robust
			representations for better performance on
			downstream tasks like instrument classification?
			<br>
			<br>
			<h2>Dataset</h2>
			To accomplish this we gather data from the Music dataset from sounds of pixels <a
				href="#MusicDataset">[5]</a>.
			The data consists of around 50 videos between 1 to 3 minutes long from 21 unique instrument classes.
			To simplify the dataset, we crop the middle 5 seconds of every video, reducing the size of the data and
			providing consistency
			across all videos. The middle is also chosen as to reduce any empty noise and select moments of playing with
			the most action.
			We also make sure to extract the audio from the video in wav format to prevent any data loss. All of these
			operations are performed using the moviepy and openCV python packages and ffmpeg commands.
			<br>
			<br>
			Using this clipped data, we can now perform augmentations to the video and audio.
			We will apply the same augmentation to each frame of a particular video for consistency. The videos will be
			augmented using two compositions of augmentations: crop + color
			and crop + sobel, as these were shown to be effective for image augmentation in contrastive learning, see <a
				href="#ChenContrastiveLearning">[3]</a>.
			The crop and color augmentations were performed using a random number to determine the amount to modify each
			parameter.
			However, for the crop size, we limit the random range from 0.5 to 1 to prevent really small cropping of
			images to
			make it impossible to identify which instrument
			is being played.
			<br>
			<br>
			Similarly, we will also apply an audio augmentation. Using common
			transformation techniques listed in the paper by Ferreira-Paiva et al. <a href="#AudioAugmentation">[4]</a>,
			we will use
			background noise to allow our representation to work for noisy data as well as volume adjustment to account
			for the distance our camera is to the source. Specifically, for the background noise, we overlay
			a quiet clapping noise in order to emulate live audience interaction with a performance.
			For the volume adjustment, we use DRC, dynamic range compression, a technique that amplifies quiet sounds
			and softens louder sounds to generate a more even sound pattern. These audio augmentations were performed 
			via the muda library in Python <a href="#MusicAugmentation">[1]</a>.
			<br>
			<br>
			Some common audio transformations we chose not to include were
			shifting pitch since pitch is correlated with the instrument type and time altering augmentation since the
			frames should correspond directly to the output sound.
			<br>
			<br>
			<h2>Encoder Models and Projection Heads</h2>
			We aim to learn a good representation of audio-visual data by generating a shared latent space
			where audio and visual data from similar instruments are mapped to the same state. To accomplish this,
			we propose the following model structure, as illustrated below,
			<br> <br>
			<img src="./images/contrastivemodel.png" width=512px />
			<ol>
				<li>Pretrained Video Encoder: 3D-ResNet18 video encoder that takes in a video and outputs a
					corresponding state
					that represents certain detected features from the video. We choose this encoder since it was shown
					to be
					effective in <a href="#ChenContrastiveLearning">[3]</a>.
				</li> <br>
				<li>
					Pretrained Audio Encoder: OpenL3 audio encoder that generates audio embeddings for the audio input
					<a href="#AudioEncoder1">[6]</a>, <a href="#AudioEncoder2">[7]</a>. This
					was chosen since OpenL3 is trained on general audio inputs compared to more popular models like
					wav2vec, which is trained on speech specifically.
				</li> <br>
				<li>
					Projection Head: A small network attached after both encoders to take their output and construct a
					shared latent space for the audio-visual input.
					We choose to make the network with three linear layers with an input and hidden dimension of 512 and
					output dimension of 128.
					Between each pair of linear layers, there is a batch normalization, a ReLU layer, and a dropout
					layer with a dropout rate of 0.1.
					This structure is chosen as to increase complexity in the network structure while preventing
					overfitting.
				</li> <br>
				<li>
					Loss Function: We use a supervised contrastive loss function <a href="#SupCon">[8]</a>.
					in order to generate similar embeddings for all audio/visual pairs for the same instrument
					instead of a single video like the loss function from Chen et al.'s paper,
					<br>
					<br>
					<math xmlns="http://www.w3.org/1998/Math/MathML" display="block">
						<mrow>
							<msub>
								<mi>l</mi>
								<mrow>
									<mi>i</mi>
									<mo>,</mo>
									<mi>j</mi>
								</mrow>
							</msub>
							<mo>=</mo>
							<mo>−</mo>
							<mfrac>
								<!-- numerator -->
								<mrow>
									<mi>exp</mi>
									<mo>(</mo>
									<mi>sim</mi>
									<mo>(</mo>
									<msub>
										<mi mathvariant="bold">z</mi>
										<mi>i</mi>
									</msub>
									<mo>,</mo>
									<msub>
										<mi mathvariant="bold">z</mi>
										<mi>j</mi>
									</msub>
									<mo>)</mo>
									<mo>/</mo>
									<mi>τ</mi>
									<mo>)</mo>
								</mrow>
								<!-- denominator -->
								<mrow>
									<munderover>
										<mo>∑</mo>
										<mrow>
											<mi>k</mi>
											<mo>=</mo>
											<mn>1</mn>
										</mrow>
										<mrow>
											<mn>2</mn>
											<mi>N</mi>
										</mrow>
									</munderover>
									<!-- indicator is 1 when A(i) ≠ A(k) -->
									<msub>
										<mi mathvariant="double-struck">1</mi>
										<mrow>
											<mo>[</mo>
											<mi>A</mi>
											<mo>(</mo>
											<mi>i</mi>
											<mo>)</mo>
											<mo>≠</mo>
											<mi>A</mi>
											<mo>(</mo>
											<mi>k</mi>
											<mo>)</mo>
											<mo>]</mo>
										</mrow>
									</msub>
									<mi>exp</mi>
									<mo>(</mo>
									<mi>sim</mi>
									<mo>(</mo>
									<msub>
										<mi mathvariant="bold">z</mi>
										<mi>i</mi>
									</msub>
									<mo>,</mo>
									<msub>
										<mi mathvariant="bold">z</mi>
										<mi>k</mi>
									</msub>
									<mo>)</mo>
									<mo>/</mo>
									<mi>τ</mi>
									<mo>)</mo>
								</mrow>
							</mfrac>
						</mrow>
					</math>
					<br>
					This function makes it so that we only calculate the sum of the similarities across different
					instruments classes instead of all different videos/audios.
				</li>
			</ol>
			We then perform a training loop with an Adam Optimizer with a learning rate of 0.0001 to train the
			projection heads. There is no need to finetune the encoders since they already detect
			the features of the input successfully, so we freeze the weights of the encoder models before performing the
			training.

			<br>
			<br>
			<h2>Classification Model</h2>
			<div class="container">
				<p class="text-content">
					<br>
					To quantify the effectiveness of the learned encodings, we train a simple linear layer with
					categorical
					cross entropy that classifies the instrument being played
					using the state generated by the contrastive model as seen in the image on the right.
					<br> <br>
					The motivation is that an encoding that effectively
					contains
					the variations across the instruments and minimizes variation within instruments
					will generate a higher accuracy output with a simple network. The linear layer will efficiently
					determine
					the simple pattern stored in the encodings compared to the more complex patterns
					stored in the encodings without the learned projection heads.
				</p>
				<img src="./images/classifcation.png" width="256px" alt="Classification Diagram" />

			</div>
			<h2>Architecture</h2>
			We perform the computation using an A100 GPU with CUDA on Google Colab.
		</div>
		<div class="margin-right-block" style="transform: translate(0%, -60%);">
			A diagram highlighting the constrative model structure. Identical projection head networks are
			added to each pretrained encoder.
		</div>
	</div>

	<div class="content-margin-container" id="analysis">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h1>Results and Analysis</h1>
			Using the dataset and the model specified in the methods, we
			run a training loop for the contrastive learning for 40 epochs.
			To visualize how successful the learned encodings are, we used scikit-learn to generate a tSNE plot with 
			perplexity 30. This procedure reduces the dimensionality of the embeddings to two dimensions while preserving 
			the relationship of the embeddings as closely as possible. This allows us to easily visualize how related
			certain embeddings are
			since embeddings that are closer together on the graph are more related.

			<br>

			<img src="./images/tSNEContrastive.png" width=512px />

			<br>

			The tSNE plot above for contrastive learning is of note because it shows some very obvious clusters of
			instruments while others are rather scattered.
			Ideally, we would want there to be a small, distinct cluster for each instrument. Some instruments like the
			erhu and electric bass
			generate these small clusters; however, instruments like the congas and ukeleles are scattered throughout
			the graph, implying that the
			model was unable to generate encodings for them that facilitate classification.

			<br> <br>

			One reason as to why this may be the case is that certain instruments have more recognizable features that
			are extracted from the video and audio encoder. For example, the erhu has a unique structure, allowing the
			contrastive learning to take advantage
			of it to generate an effective encoding. Therefore, from these encodings,
			we would expect the classification model to perform better on certain instrument classes over others.
			Let's see how the classification performs on the data.

			<br><br>

			<div class="row">
				<div class="column">
					<img src="./images/contrastive_loss.png" alt="contrastive_loss" style="width:100%">
				</div>
				<div class="column">
					<img src="./images/contrastive_accuracy.png" alt="contrastive_accuracy" style="width:100%">
				</div>
			</div>

			<br><br>

			As we can see from the graphs above, the model converges near 20 epochs and ends with a final classification
			accuracy of around 80%.
			However, to truly understand if the contrastive learning algorithm performs better, we need to compare it
			with taking
			the encoding
			without the projection heads. This will tell us how well current encoders perform without contrastive
			learning on augmentations.
			Below, we have a table summarizing the results from contrastive loss vs. regular encodings on
			combinations of training and validation types of
			data (original, augmented, or both). The accuracies are the maximimum validation accuracy
			achieved by the models during training over 20 epochs.
			<br><br>

			<table class="table">
				<tr class="tr">
					<th class="th">Train/Validation</th>
					<th class="th">Contrastive Encodings</th>
					<th class="th">Regular Encodings</th>
				</tr>
				<tr class="tr">
					<th class="th"> Augmented/Augmented </th>
					<td class="td">84.47%</td>
					<td class="td">76.70%</td>
				</tr>
				<tr class="tr">
					<th class="th"> Unaugmented/Augmented </th>
					<td class="td">81.23%</td>
					<td class="td">76.70%</td>
				</tr>
				<tr class="tr">
					<th class="th"> Both/Unaugmented </th>
					<td class="td">93.85%</td>
					<td class="td">92.56%</td>
				</tr>
			</table>

			<br><br>

			From the data, we notice a few interesting results. Firstly, the max accuracies obtained by the encodings
			generated by contrastive loss are higher than the encodings taken directly from ResNet-18 and OpenL3.
			Specifically, there is a significant improvement in classification accuracy when validated with augmented
			data but
			trained with augmented or unaugmented data. This makes sense because the contrastive loss took into account
			augmentations
			on the input data, so it generated encodings that contained patterns found in the augmented data. Therefore,
			when
			training a simple linear layer on any data and validating with the augmented data, the model was able to
			identify patterns easier in the encodings from contrastive loss
			compared to regular encodings.

			<br> <br>

			This aligns decently well with the tSNE plot for contrastive learning. If we take a look at which instrument
			classes had the highest
			accuracy of being measured for contrastive encoding and augmented training and validation, we noticed that the accordion,
			bagpipe, drum, piano, and violin all had 100% accuracies in classification. This matches up nicely with small clusters for each
			of those instruments present in the tSNE plot. However, the saxophone had 0% classification accuracy, which lines up with
			the observed random scattering of saxophone points in the tSNE plot. Therefore, we can see that as expected, the instruments whose encodings
			were clustered in the tSNE plot, have a high similarity, were able to be classified significantly more accurately compared to instruments
			with no pattern, spread out throughout the tSNE plot. One reason for this high variance in instrument encoding is that the augmentations
			created more significant variation for some instruments than others. We propose this hypothesis because the saxophone had a 80% classification accuracy
			for the encodings produced without contrastive learning.

			<br> <br>

			Something else worth noting is the last row of the table. There appears to be no significant difference when
			trained with the full dataset
			but validated with unaugmented data. This means that for unaugmented data, the original encoders already do
			a good job in identifying features in the
			audio-visual data. Therefore, performing contrastive learning isn't necessary here for testing on unaugmented data since the
			representations
			generated already suffice for classification.


		</div>
		<div class="margin-right-block" style="transform: translate(0%, 0%);">
			A tSNE plot visualizing the relationship between the embeddings generated by contrastive learning.
			<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
			<br><br><br><br><br><br><br><br><br><br><br><br><br>
			The left graph is the loss of the classification model over 20 epochs and the right graph is the accuracy
			of a validation dataset at each epoch. We use a 70-30 split for the training/validation data.
		</div>
	</div>

	<div class="content-margin-container" id="conclusion">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<h1>Conclusion</h1>
			In this post, we took a look at how contrastive learning with augmentations on audio-visual data aids in
			generating better representations
			for classification of real world data. We did this by generating a model that attaches a projection head to
			a video and audio encoders and
			trains it with a contrastive loss. We then benchmarked the representations generated by the model with
			representations outputted by just the
			encoders. This provided insight into ... (TODO: finish this once analysis is done)

			<br><br>

			Using this analysis, we conclude that contrastive learning may be beneficial for generating representations
			that
			encode real world audio-visual data through particular augmentations. With better augmentations,

			<br> <br>

			These encodings are powerful because not only do they provide a way to represent real world data, they do it
			in a compact manner, reducing the size
			of data. The hope would be to generate an encoding procedure that is compact but also accurately contains
			the complexities in the data so that
			classification can be done through a simple linear layer with minimal training.

			<br><br>

			For some future directions, it may be worth properly understanding the adapatibility of the contrastive
			representations.
			By this, we mean applying new augmentations to data and quantifying the effectiveness of the learned
			representation.
			Ideally, these representation would be able to adapt to arbitrary representations, making it useful for a
			wide array of real
			world applications.

			<br><br>

			For the complete code and model used for this paper, please visit our <a
				href="https://github.com/shah-rushil/6.8300-Project">GitHub</a>.
		</div>
		<div class="margin-right-block">
		</div>
	</div>

	<div class="content-margin-container" id="citations">
		<div class="margin-left-block">
		</div>
		<div class="main-content-block">
			<div class='citation' id="references" style="height:auto"><br>
				<span style="font-size:18px; font-weight: bold;">References:</span><br><br>
				<a id="MusicAugmentation"></a>[1] A Software Framework for Musical Data Augmentation,
				Brian McFee, Eric J. Humphrey, and Juan Pablo Bello,
				in <i>16th International Society for Music Information Retrieval Conference (ISMIR)</i>, 2015<br><br>
				<a id="AudioVisualConstrastiveLearning"></a>[2] <a href="https://arxiv.org/abs/2009.09805">Active
					Contrastive Learning of Audio-Visual Video Representations</a>, Shuang Ma and Zhaoyang Zeng and
				Daniel McDuff and Yale Song, 2021<br><br>
				<a id="ChenContrastiveLearning"></a>[3] <a href="https://arxiv.org/abs/2002.05709">A Simple Framework
					for Contrastive Learning of Visual Representations</a>, Ting Chen and Simon Kornblith and Mohammad
				Norouzi and Geoffrey Hinton, 2020<br><br>
				<a id="AudioAugmentation"></a>[4] <a
					href="https://sba.org.br/open_journal_systems/index.php/cba/article/view/3469">A Survey of Data
					Augmentation for Audio Classification</a>, Ferreira-Paiva, Lucas and Alfaro Espinoza, Elizabeth and
				Martins Almeida, Vinicius and Felix, Leonardo and Neves, Rodolpho, 2022<br><br>
				<a id="MusicDataset"></a>[5] <a href="https://github.com/roudimit/MUSIC_dataset/">MUSIC Dataset from
					Sound of Pixels</a>, Andrew Rouditchenko and Hang Zhao, 2020<br><br>
				<a id="AudioEncoder1"></a>[6] <a
					href="https://www.justinsalamon.com/uploads/4/3/9/4/4394963/cramer_looklistenlearnmore_icassp_2019.pdf">Look,
					Listen and Learn More: Design Choices for Deep Audio Embeddings</a>
				Aurora Cramer, Ho-Hsiang Wu, Justin Salamon, and Juan Pablo Bello, 2019. <br><br>
				<a id="AudioEncoder2"></a>[7] <a
					href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Arandjelovic_Look_Listen_and_ICCV_2017_paper.pdf">Look,
					Listen and Learn</a>
				Relja Arandjelović and Andrew Zisserman, 2017. <br><br>
				<a id="SupCon"></a>[8] <a href="https://arxiv.org/abs/2004.11362">Supervised Contrastive Learning</a>,
				Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola,
				Aaron Maschinot, Ce Liu, and Dilip Krishnan, 2021<br><br>
			</div>
		</div>
		<div class="margin-right-block">
			<!-- margin notes for reference block here -->
		</div>
	</div>

</body>

</html>